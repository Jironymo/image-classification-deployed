{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preliminary step: import libs, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f804ce2fe10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "MLFLOW_SERVER_URL = 'http://127.0.0.1:5005/'\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "SEED=42\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"y\" | unzip ../../assets/fashion-mnist.zip -d ../../assets/ &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"../../assets/fashion-mnist_train.csv\")\n",
    "test_csv = pd.read_csv(\"../../assets/fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_csv['label'].values\n",
    "X_train = train_csv.drop(['label'],axis=1).values\n",
    "\n",
    "y_test = test_csv['label'].values\n",
    "X_test = test_csv.drop(['label'],axis=1).values\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduce teacher model to be distilled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, epoch_number=5, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epoch_number):\n",
    "        correct = 0\n",
    "        \n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "\n",
    "def evaluate(model):\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        \n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    score = round(float(correct) / (len(test_loader)*BATCH_SIZE), 3)\n",
    "    # print(\"Test accuracy:{}% \".format(score))\n",
    "    return score\n",
    "\n",
    "\n",
    "def calc_weights(model):\n",
    "    result = 0\n",
    "    for layer in model.children():\n",
    "        result += len(layer.weight.reshape(-1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/opt/mlflow_docker/mlflow_server/2', creation_time=1675984716425, experiment_id='2', last_update_time=1675984716425, lifecycle_stage='active', name='distill_torch_teacher', tags={}>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# подключаемся к серверу\n",
    "mlflow.set_tracking_uri(MLFLOW_SERVER_URL)\n",
    "\n",
    "experiment_name = 'distill_torch_teacher'\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/60000 (0%)]\tLoss: 20.433893\t Accuracy:0.000%\n",
      "Epoch : 0 [6400/60000 (11%)]\tLoss: 0.715844\t Accuracy:69.745%\n",
      "Epoch : 0 [12800/60000 (21%)]\tLoss: 0.828387\t Accuracy:74.135%\n",
      "Epoch : 0 [19200/60000 (32%)]\tLoss: 0.356695\t Accuracy:75.816%\n",
      "Epoch : 0 [25600/60000 (43%)]\tLoss: 0.280699\t Accuracy:76.990%\n",
      "Epoch : 0 [32000/60000 (53%)]\tLoss: 0.561405\t Accuracy:77.700%\n",
      "Epoch : 0 [38400/60000 (64%)]\tLoss: 0.367472\t Accuracy:78.367%\n",
      "Epoch : 0 [44800/60000 (75%)]\tLoss: 0.309912\t Accuracy:78.928%\n",
      "Epoch : 0 [51200/60000 (85%)]\tLoss: 0.222573\t Accuracy:79.318%\n",
      "Epoch : 0 [57600/60000 (96%)]\tLoss: 0.184787\t Accuracy:79.555%\n",
      "Epoch : 1 [0/60000 (0%)]\tLoss: 0.460531\t Accuracy:87.500%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 0.524790\t Accuracy:83.178%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 0.705526\t Accuracy:83.611%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 0.275794\t Accuracy:83.907%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 0.260962\t Accuracy:84.028%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 0.452591\t Accuracy:83.950%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 0.458119\t Accuracy:84.000%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 0.255401\t Accuracy:84.147%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 0.164242\t Accuracy:84.256%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 0.193550\t Accuracy:84.309%\n",
      "Epoch : 2 [0/60000 (0%)]\tLoss: 0.367968\t Accuracy:87.500%\n",
      "Epoch : 2 [6400/60000 (11%)]\tLoss: 0.511334\t Accuracy:84.997%\n",
      "Epoch : 2 [12800/60000 (21%)]\tLoss: 0.634879\t Accuracy:85.209%\n",
      "Epoch : 2 [19200/60000 (32%)]\tLoss: 0.275268\t Accuracy:85.207%\n",
      "Epoch : 2 [25600/60000 (43%)]\tLoss: 0.226621\t Accuracy:85.327%\n",
      "Epoch : 2 [32000/60000 (53%)]\tLoss: 0.548604\t Accuracy:85.287%\n",
      "Epoch : 2 [38400/60000 (64%)]\tLoss: 0.440198\t Accuracy:85.314%\n",
      "Epoch : 2 [44800/60000 (75%)]\tLoss: 0.249537\t Accuracy:85.341%\n",
      "Epoch : 2 [51200/60000 (85%)]\tLoss: 0.143486\t Accuracy:85.384%\n",
      "Epoch : 2 [57600/60000 (96%)]\tLoss: 0.217042\t Accuracy:85.326%\n",
      "Epoch : 3 [0/60000 (0%)]\tLoss: 0.359673\t Accuracy:90.625%\n",
      "Epoch : 3 [6400/60000 (11%)]\tLoss: 0.363903\t Accuracy:85.852%\n",
      "Epoch : 3 [12800/60000 (21%)]\tLoss: 0.548987\t Accuracy:85.848%\n",
      "Epoch : 3 [19200/60000 (32%)]\tLoss: 0.229087\t Accuracy:85.971%\n",
      "Epoch : 3 [25600/60000 (43%)]\tLoss: 0.227218\t Accuracy:85.959%\n",
      "Epoch : 3 [32000/60000 (53%)]\tLoss: 0.538147\t Accuracy:85.905%\n",
      "Epoch : 3 [38400/60000 (64%)]\tLoss: 0.410834\t Accuracy:85.798%\n",
      "Epoch : 3 [44800/60000 (75%)]\tLoss: 0.267883\t Accuracy:85.823%\n",
      "Epoch : 3 [51200/60000 (85%)]\tLoss: 0.140069\t Accuracy:85.897%\n",
      "Epoch : 3 [57600/60000 (96%)]\tLoss: 0.118019\t Accuracy:85.845%\n",
      "Epoch : 4 [0/60000 (0%)]\tLoss: 0.270143\t Accuracy:93.750%\n",
      "Epoch : 4 [6400/60000 (11%)]\tLoss: 0.441709\t Accuracy:86.210%\n",
      "Epoch : 4 [12800/60000 (21%)]\tLoss: 0.563859\t Accuracy:86.191%\n",
      "Epoch : 4 [19200/60000 (32%)]\tLoss: 0.197155\t Accuracy:86.236%\n",
      "Epoch : 4 [25600/60000 (43%)]\tLoss: 0.203800\t Accuracy:86.408%\n",
      "Epoch : 4 [32000/60000 (53%)]\tLoss: 0.456018\t Accuracy:86.326%\n",
      "Epoch : 4 [38400/60000 (64%)]\tLoss: 0.460134\t Accuracy:86.392%\n",
      "Epoch : 4 [44800/60000 (75%)]\tLoss: 0.276290\t Accuracy:86.440%\n",
      "Epoch : 4 [51200/60000 (85%)]\tLoss: 0.170870\t Accuracy:86.462%\n",
      "Epoch : 4 [57600/60000 (96%)]\tLoss: 0.132560\t Accuracy:86.426%\n"
     ]
    }
   ],
   "source": [
    "# подключаемся к серверу\n",
    "mlflow.set_tracking_uri(MLFLOW_SERVER_URL)\n",
    "\n",
    "experiment_name = 'distill_torch_teacher'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# запуск в эксперименте\n",
    "with mlflow.start_run():\n",
    "    epochs = 5\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # модель\n",
    "    mlp = MLP()\n",
    "    fit(mlp, train_loader, epochs, lr=learning_rate)\n",
    "    \n",
    "    # метрики\n",
    "    n_weights = calc_weights(mlp)\n",
    "    accuracy = evaluate(mlp)\n",
    "\n",
    "    # сохраняем значения эксперимента в системе\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"n_weights\", n_weights)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    mlflow.pytorch.log_model(mlp, \"model\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Add teacher model to registry, serve a teacher model for \"why not?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient(MLFLOW_SERVER_URL)\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "# run_info = client.list_run_infos(experiment.experiment_id)[-1] # not working for some reason. Deprecated?\n",
    "# mlflow.last_active_run().data # alternatively. ids mismatch though, yet queries correspond (strangely)\n",
    "last_run_info = mlflow.search_runs([experiment.experiment_id]).iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/02/10 02:31:12 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: distill_teacher_MLP, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ModelVersion: creation_timestamp=1675985472718, current_stage='None', description='', last_updated_timestamp=1675985472718, name='distill_teacher_MLP', run_id='36ee55666a9a420e9e182325256190c0', run_link='', source='/opt/mlflow_docker/mlflow_server/2/36ee55666a9a420e9e182325256190c0/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='1'>\n"
     ]
    }
   ],
   "source": [
    "reg_model_name = \"distill_teacher_MLP\"\n",
    "\n",
    "# региструем модель\n",
    "client.create_registered_model(reg_model_name)\n",
    "# создаем первую версию\n",
    "result = client.create_model_version(\n",
    "    name=reg_model_name,\n",
    "    source=f\"{last_run_info['artifact_uri']}/model\",\n",
    "    run_id=last_run_info['run_id']\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1675985472718, current_stage='Staging', description='', last_updated_timestamp=1675985491412, name='distill_teacher_MLP', run_id='36ee55666a9a420e9e182325256190c0', run_link='', source='/opt/mlflow_docker/mlflow_server/2/36ee55666a9a420e9e182325256190c0/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.transition_model_version_stage(\n",
    "    name=reg_model_name,\n",
    "    version=1,\n",
    "    stage=\"Staging\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/02/10 02:33:04 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "2023/02/10 02:33:04 INFO mlflow.pyfunc.backend: === Running command 'exec gunicorn --timeout=60 -b 127.0.0.1:5016 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n",
      "[2023-02-10 02:33:05 +0300] [67269] [INFO] Starting gunicorn 20.1.0\n",
      "[2023-02-10 02:33:05 +0300] [67269] [INFO] Listening at: http://127.0.0.1:5016 (67269)\n",
      "[2023-02-10 02:33:05 +0300] [67269] [INFO] Using worker: sync\n",
      "[2023-02-10 02:33:05 +0300] [67272] [INFO] Booting worker with pid: 67272\n"
     ]
    }
   ],
   "source": [
    "# seems like needs to be run on the mlflow_server machine. Other CLI-native options?\n",
    "os.system(f'MLFLOW_TRACKING_URI=http://127.0.0.1:5005/ mlflow models serve -m \"models:/{reg_model_name}/Staging\" -p 5016 --no-conda &')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing connection and input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "test = iter(test_loader)\n",
    "test_imgs,  = next(test)\n",
    "img0 = Variable(test_imgs).float()[0]\n",
    "\n",
    "url = f'http://127.0.0.1:5016/invocations'\n",
    "\n",
    "data = pd.DataFrame(img0.reshape([1, 784])).to_dict(orient='split')\n",
    "\n",
    "http_data = json.dumps({\"dataframe_split\": data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: {\"predictions\": [{\"0\": 6.712867736816406, \"1\": -5.011413097381592, \"2\": 0.6917873620986938, \"3\": 0.9353787302970886, \"4\": -0.2633971571922302, \"5\": -15.808575630187988, \"6\": 7.237793445587158, \"7\": -22.36628532409668, \"8\": -0.10114485025405884, \"9\": -11.855427742004395}]}\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(url=url, headers={'Content-Type': 'application/json'}, data=http_data)\n",
    "\n",
    "print(f'Predictions: {response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = response.json()['predictions'][0]\n",
    "int(max(r, key = r.get))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Introduce a lightweight student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784,16)\n",
    "        self.linear2 = nn.Linear(16,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = self.linear2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12704"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smlp_simple = StudentMLP()\n",
    "calc_weights(smlp_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill(teacher_model, student_model, train_loader, epoch_number=5, alpha=0.5, temperature=2, lr=1e-3):\n",
    "    def error_and_output(var_X_batch, var_y_batch): # Задаем нашу особую функцию ошибки\n",
    "        # Дивергенция Кульбака-Лейблера нужна, чтобы подсчитать кросс-энтропию между двумя распределениями\n",
    "        # А именно между распределениями ответов модели-учителя и модели-ученика\n",
    "        kldloss = nn.KLDivLoss()  \n",
    "        # Для подсчета ошибки на данных воспользуемся уже готовой функцией для кросс-энтропии\n",
    "        celoss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Считаем выходы из сети-учителя\n",
    "        teacher_logits = teacher_model(var_X_batch)\n",
    "        # И выходы из сети-ученика\n",
    "        student_logits = student_model(var_X_batch)\n",
    "        \n",
    "        # Рассчитываем распределение вероятностей ответов с помощью softmax с параметром T для сети-ученика\n",
    "        soft_predictions = F.log_softmax( student_logits / temperature, dim=1 )\n",
    "        # И для сети-учителя\n",
    "        soft_labels = F.softmax( teacher_logits / temperature, dim=1 )\n",
    "        # Считаем ошибку дистиляции - кросс-энтропию между распределениями ответов моделей\n",
    "        distillation_loss = kldloss(soft_predictions, soft_labels)\n",
    "        \n",
    "        # Считаем ошибку на данных - кросс-энтропию между распределением ответов сети-ученика и правильным ответом\n",
    "        student_loss = celoss(student_logits, var_y_batch)\n",
    "        \n",
    "        # Складываем с весами\n",
    "        return distillation_loss * alpha + student_loss * (1 - alpha), student_logits\n",
    "    \n",
    "    optimizer = torch.optim.Adam(student_model.parameters(), lr=lr)\n",
    "    student_model.train()\n",
    "    \n",
    "    # Далее обучение проходит как обычно\n",
    "    for epoch in range(epoch_number):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            \n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss, output = error_and_output(var_X_batch, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.data, float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2, 5, 5, 0.001)\n",
      "(0.2, 10, 5, 0.001)\n",
      "(0.8, 5, 5, 0.001)\n",
      "(0.8, 10, 5, 0.001)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "alpha_list = [0.2, 0.8, ]\n",
    "temperature_list = [5, 10,]\n",
    "epochs_list = [5,]\n",
    "lr_list = [1e-3,]\n",
    "\n",
    "for element in itertools.product(alpha_list, temperature_list, epochs_list, lr_list):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подключаемся к серверу\n",
    "mlflow.set_tracking_uri(MLFLOW_SERVER_URL)\n",
    "\n",
    "experiment_name = 'distill_torch_student'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "import itertools\n",
    "\n",
    "alpha_list = [0.2, 0.8, ]\n",
    "temperature_list = [5, 10,]\n",
    "epochs_list = [5,]\n",
    "lr_list = [1e-3,]\n",
    "\n",
    "for element in itertools.product(alpha_list, temperature_list, epochs_list, lr_list):\n",
    "\n",
    "    # запуск в эксперименте\n",
    "    with mlflow.start_run():\n",
    "        alpha, temperature, epochs, learning_rate = element\n",
    "\n",
    "        # модель\n",
    "        smlp = StudentMLP()\n",
    "        distill(mlp, smlp, train_loader, epoch_number=epochs ,temperature=temperature, alpha=alpha, lr=learning_rate)\n",
    "        \n",
    "        # метрики\n",
    "        n_weights = calc_weights(smlp)\n",
    "        accuracy = evaluate(smlp)\n",
    "\n",
    "        # сохраняем значения эксперимента в системе\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"n_weights\", n_weights)\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_param(\"temperature\", temperature)\n",
    "        \n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        \n",
    "        mlflow.pytorch.log_model(smlp, \"model\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Find the best model, add it to production stage, then mlflow serve it  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient(MLFLOW_SERVER_URL)\n",
    "experiment = client.get_experiment_by_name(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_info = client.list_run_infos(experiment.experiment_id)[-1] # not working for some reason. Deprecated?\n",
    "# mlflow.last_active_run().data # alternatively. ids mismatch though, yet queries correspond (strangely)\n",
    "runs_info_df = mlflow.search_runs([experiment.experiment_id])\n",
    "best_run_info = runs_info_df.iloc[runs_info_df['metrics.accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_id                                            b130105f2c964c4dbe723c0ad1bbaf72\n",
       "experiment_id                                                                    3\n",
       "status                                                                    FINISHED\n",
       "artifact_uri                     /opt/mlflow_docker/mlflow_server/3/b130105f2c9...\n",
       "start_time                                        2023-02-10 00:49:18.185000+00:00\n",
       "end_time                                          2023-02-10 00:49:52.877000+00:00\n",
       "metrics.accuracy                                                             0.742\n",
       "params.alpha                                                                   0.2\n",
       "params.epochs                                                                    5\n",
       "params.n_weights                                                             12704\n",
       "params.temperature                                                               5\n",
       "tags.mlflow.runName                                               colorful-ram-912\n",
       "tags.mlflow.user                                                          jironymo\n",
       "tags.mlflow.log-model.history    [{\"run_id\": \"b130105f2c964c4dbe723c0ad1bbaf72\"...\n",
       "tags.mlflow.source.name          /home/jironymo/.local/lib/python3.8/site-packa...\n",
       "tags.mlflow.source.type                                                      LOCAL\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/02/10 04:02:39 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: distill_student_MLP, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ModelVersion: creation_timestamp=1675990959258, current_stage='None', description='', last_updated_timestamp=1675990959258, name='distill_student_MLP', run_id='36ee55666a9a420e9e182325256190c0', run_link='', source='/opt/mlflow_docker/mlflow_server/2/36ee55666a9a420e9e182325256190c0/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='1'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1675990959258, current_stage='Staging', description='', last_updated_timestamp=1675990959276, name='distill_student_MLP', run_id='36ee55666a9a420e9e182325256190c0', run_link='', source='/opt/mlflow_docker/mlflow_server/2/36ee55666a9a420e9e182325256190c0/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model_name = \"distill_student_MLP\"\n",
    "\n",
    "# региструем модель\n",
    "client.create_registered_model(reg_model_name)\n",
    "# создаем первую версию\n",
    "result = client.create_model_version(\n",
    "    name=reg_model_name,\n",
    "    source=f\"{last_run_info['artifact_uri']}/model\",\n",
    "    run_id=last_run_info['run_id']\n",
    ")\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name=reg_model_name,\n",
    "    version=1,\n",
    "    stage=\"Staging\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/02/10 05:06:41 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "2023/02/10 05:06:41 INFO mlflow.pyfunc.backend: === Running command 'exec gunicorn --timeout=60 -b 127.0.0.1:5017 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n",
      "[2023-02-10 05:06:42 +0300] [93527] [INFO] Starting gunicorn 20.1.0\n",
      "[2023-02-10 05:06:42 +0300] [93527] [INFO] Listening at: http://127.0.0.1:5017 (93527)\n",
      "[2023-02-10 05:06:42 +0300] [93527] [INFO] Using worker: sync\n",
      "[2023-02-10 05:06:42 +0300] [93528] [INFO] Booting worker with pid: 93528\n",
      "2023/02/10 05:06:43 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - cloudpickle (current: 2.2.1, required: cloudpickle==2.0.0)\n",
      " - ipython (current: 8.9.0, required: ipython==8.5.0)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2023/02/10 05:06:43 WARNING mlflow.pyfunc: The version of Python that the model was saved in, `Python 3.8.8`, differs from the version of Python that is currently running, `Python 3.10.9`, and may be incompatible\n"
     ]
    }
   ],
   "source": [
    "reg_model_name = \"distill_student_MLP\"\n",
    "# seems like needs to be run on the mlflow_server machine. Other CLI-native options?\n",
    "os.system(f'MLFLOW_TRACKING_URI=http://127.0.0.1:5005/ mlflow models serve -m \"models:/{reg_model_name}/Staging\" -p 5017 --no-conda &')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test served small MLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = f'http://127.0.0.1:5017/invocations'\n",
    "data = pd.DataFrame(img0.reshape([1, 784])).to_dict(orient='split')\n",
    "http_data = json.dumps({\"dataframe_split\": data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: {\"predictions\": [{\"0\": 6.712867736816406, \"1\": -5.011413097381592, \"2\": 0.6917873620986938, \"3\": 0.9353787302970886, \"4\": -0.2633971571922302, \"5\": -15.808575630187988, \"6\": 7.237793445587158, \"7\": -22.36628532409668, \"8\": -0.10114485025405884, \"9\": -11.855427742004395}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = requests.post(url=url, headers={'Content-Type': 'application/json'}, data=http_data)\n",
    "\n",
    "print(f'Predictions: {response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = response.json()['predictions'][0]\n",
    "int(max(r, key = r.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed93dbe742fd3ab42c02e81cdee14c7f1d09970af698aa2d762d61a7e26400d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
